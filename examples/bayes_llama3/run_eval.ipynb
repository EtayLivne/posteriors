{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/miniconda/envs/posteriors/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.03it/s]\n",
      "Some weights of BayesLlamaForCausalLM were not initialized from the model checkpoint at Meta-Llama-3-8B and are newly initialized: ['model.bayesian_layers.0.input_layernorm.weight', 'model.bayesian_layers.0.mlp.down_proj.weight', 'model.bayesian_layers.0.mlp.gate_proj.weight', 'model.bayesian_layers.0.mlp.up_proj.weight', 'model.bayesian_layers.0.post_attention_layernorm.weight', 'model.bayesian_layers.0.self_attn.k_proj.weight', 'model.bayesian_layers.0.self_attn.o_proj.weight', 'model.bayesian_layers.0.self_attn.q_proj.weight', 'model.bayesian_layers.0.self_attn.v_proj.weight', 'model.bayesian_layers.1.input_layernorm.weight', 'model.bayesian_layers.1.mlp.down_proj.weight', 'model.bayesian_layers.1.mlp.gate_proj.weight', 'model.bayesian_layers.1.mlp.up_proj.weight', 'model.bayesian_layers.1.post_attention_layernorm.weight', 'model.bayesian_layers.1.self_attn.k_proj.weight', 'model.bayesian_layers.1.self_attn.o_proj.weight', 'model.bayesian_layers.1.self_attn.q_proj.weight', 'model.bayesian_layers.1.self_attn.v_proj.weight', 'model.bayesian_layers.2.input_layernorm.weight', 'model.bayesian_layers.2.mlp.down_proj.weight', 'model.bayesian_layers.2.mlp.gate_proj.weight', 'model.bayesian_layers.2.mlp.up_proj.weight', 'model.bayesian_layers.2.post_attention_layernorm.weight', 'model.bayesian_layers.2.self_attn.k_proj.weight', 'model.bayesian_layers.2.self_attn.o_proj.weight', 'model.bayesian_layers.2.self_attn.q_proj.weight', 'model.bayesian_layers.2.self_attn.v_proj.weight', 'model.bayesian_layers.3.input_layernorm.weight', 'model.bayesian_layers.3.mlp.down_proj.weight', 'model.bayesian_layers.3.mlp.gate_proj.weight', 'model.bayesian_layers.3.mlp.up_proj.weight', 'model.bayesian_layers.3.post_attention_layernorm.weight', 'model.bayesian_layers.3.self_attn.k_proj.weight', 'model.bayesian_layers.3.self_attn.o_proj.weight', 'model.bayesian_layers.3.self_attn.q_proj.weight', 'model.bayesian_layers.3.self_attn.v_proj.weight', 'model.bayesian_layers.4.input_layernorm.weight', 'model.bayesian_layers.4.mlp.down_proj.weight', 'model.bayesian_layers.4.mlp.gate_proj.weight', 'model.bayesian_layers.4.mlp.up_proj.weight', 'model.bayesian_layers.4.post_attention_layernorm.weight', 'model.bayesian_layers.4.self_attn.k_proj.weight', 'model.bayesian_layers.4.self_attn.o_proj.weight', 'model.bayesian_layers.4.self_attn.q_proj.weight', 'model.bayesian_layers.4.self_attn.v_proj.weight', 'model.bayesian_layers.5.input_layernorm.weight', 'model.bayesian_layers.5.mlp.down_proj.weight', 'model.bayesian_layers.5.mlp.gate_proj.weight', 'model.bayesian_layers.5.mlp.up_proj.weight', 'model.bayesian_layers.5.post_attention_layernorm.weight', 'model.bayesian_layers.5.self_attn.k_proj.weight', 'model.bayesian_layers.5.self_attn.o_proj.weight', 'model.bayesian_layers.5.self_attn.q_proj.weight', 'model.bayesian_layers.5.self_attn.v_proj.weight', 'model.bayesian_layers.6.input_layernorm.weight', 'model.bayesian_layers.6.mlp.down_proj.weight', 'model.bayesian_layers.6.mlp.gate_proj.weight', 'model.bayesian_layers.6.mlp.up_proj.weight', 'model.bayesian_layers.6.post_attention_layernorm.weight', 'model.bayesian_layers.6.self_attn.k_proj.weight', 'model.bayesian_layers.6.self_attn.o_proj.weight', 'model.bayesian_layers.6.self_attn.q_proj.weight', 'model.bayesian_layers.6.self_attn.v_proj.weight', 'model.bayesian_layers.7.input_layernorm.weight', 'model.bayesian_layers.7.mlp.down_proj.weight', 'model.bayesian_layers.7.mlp.gate_proj.weight', 'model.bayesian_layers.7.mlp.up_proj.weight', 'model.bayesian_layers.7.post_attention_layernorm.weight', 'model.bayesian_layers.7.self_attn.k_proj.weight', 'model.bayesian_layers.7.self_attn.o_proj.weight', 'model.bayesian_layers.7.self_attn.q_proj.weight', 'model.bayesian_layers.7.self_attn.v_proj.weight', 'model.bayesian_layers.8.input_layernorm.weight', 'model.bayesian_layers.8.mlp.down_proj.weight', 'model.bayesian_layers.8.mlp.gate_proj.weight', 'model.bayesian_layers.8.mlp.up_proj.weight', 'model.bayesian_layers.8.post_attention_layernorm.weight', 'model.bayesian_layers.8.self_attn.k_proj.weight', 'model.bayesian_layers.8.self_attn.o_proj.weight', 'model.bayesian_layers.8.self_attn.q_proj.weight', 'model.bayesian_layers.8.self_attn.v_proj.weight', 'model.bayesian_layers.9.input_layernorm.weight', 'model.bayesian_layers.9.mlp.down_proj.weight', 'model.bayesian_layers.9.mlp.gate_proj.weight', 'model.bayesian_layers.9.mlp.up_proj.weight', 'model.bayesian_layers.9.post_attention_layernorm.weight', 'model.bayesian_layers.9.self_attn.k_proj.weight', 'model.bayesian_layers.9.self_attn.o_proj.weight', 'model.bayesian_layers.9.self_attn.q_proj.weight', 'model.bayesian_layers.9.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading ensemble weights: 100%|██████████| 10/10 [00:00<00:00, 12.76it/s]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from llama3.eval import Experiment\n",
    "from llama3.utils.load_utils import load_config, save_config, setup_log_dir\n",
    "\n",
    "config = load_config(\"configs/bayes_eval.yaml\")\n",
    "\n",
    "experiment_log_dir = setup_log_dir(\n",
    "    config.get(\"logs_dir\", \"experiment_logs\"), experiment_name=config.get(\"experiment_name\", None)\n",
    ")\n",
    "\n",
    "config[\"experiment_config\"][\"experiment_log_dir\"] = experiment_log_dir\n",
    "save_config(config.to_dict(), experiment_log_dir + \"/config.yaml\")\n",
    "\n",
    "experiment = Experiment(config[\"experiment_config\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/210 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following multiple choice question very succintly.\n",
      "\n",
      "Steps of the scientific method include all of the following except\n",
      "a. doing background research.\n",
      "b. constructing a hypothesis.\n",
      "c. asking a question.\n",
      "d. proving a theory.\n",
      "\n",
      "Answer:\n",
      "Answer the following multiple choice question very succintly.\n",
      "\n",
      "Why do scientists call the Big Bang a theory?\n",
      "a. It is probably unlikely and therefore not a fact.\n",
      "b. A very well respected scientist proved it to be true.\n",
      "c. Many scientists have agreed upon this explanation after repeated experiments and models have shown it\n",
      "d. All possible answers to a scientific idea are called theories.\n",
      "\n",
      "Answer:\n",
      "Answer the following multiple choice question very succintly.\n",
      "\n",
      "The data collected in an experiment should always be\n",
      "a. labeled.\n",
      "b. recorded.\n",
      "c. reported.\n",
      "d. all of the above\n",
      "\n",
      "Answer:\n",
      "Answer the following multiple choice question very succintly.\n",
      "\n",
      "Which of the following is not a scientific model?\n",
      "a. A cross section of an apple that mimics the layers of the Earth.\n",
      "b. A chart with nutritional information about food we eat.\n",
      "c. A computer simulation that can show what will happen to algae in a pond over 10 years given conditions\n",
      "d. An explanation for the extinction of the dinosaurs that takes into account volcanic activity, climate, space\n",
      "\n",
      "Answer:\n",
      "Answer the following multiple choice question very succintly.\n",
      "\n",
      "If the results of an experiment disprove a hypothesis, then the\n",
      "a. results should not be reported.\n",
      "b. hypothesis is just a theory.\n",
      "c. data must contain errors.\n",
      "d. none of the above\n",
      "\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/210 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' The correct option is (', ' The Big Bang is called a theory because many scientists', ' d', ' The correct answer is option b. A chart with', ' d']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.chat_model=False\n",
    "experiment.run(config[\"dataset_path\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The correct option isFigure ',\n",
       " 'The correct option is d. All possible answers to a scientific idea are called theories.  The Big',\n",
       " 'd',\n",
       " 'd.  ://://Figure  Figure ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs_a = \"\"\"\n",
    "Answer the following multiple choice question as succintly as possible.\n",
    "\n",
    "Steps of the scientific method include all of the following except\n",
    "a. doing background research.\n",
    "b. constructing a hypothesis.\n",
    "c. asking a question.\n",
    "d. proving a theory.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "inputs_b = \"\"\"\n",
    "Answer the following multiple choice question as succintly as possible.\n",
    "\n",
    "Why do scientists call the Big Bang a theory?\n",
    "a. It is probably unlikely and therefore not a fact.\n",
    "b. A very well respected scientist proved it to be true.\n",
    "c. Many scientists have agreed upon this explanation after repeated experiments and models have shown it\n",
    "d. All possible answers to a scientific idea are called theories.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "inputs_c = \"\"\"\n",
    "Answer the following multiple choice question as succintly as possible.\n",
    "\n",
    "The data collected in an experiment should always be\n",
    "a. labeled.\n",
    "b. recorded.\n",
    "c. reported.\n",
    "d. all of the above\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "inputs_d = \"\"\"\n",
    "Answer the following multiple choice question as succintly as possible.\n",
    "\n",
    "If the results of an experiment disprove a hypothesis, then the\n",
    "a. results should not be reported.\n",
    "b. hypothesis is just a theory.\n",
    "c. data must contain errors.\n",
    "d. none of the above\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, inputs, max_length=20, use_cache=True):\n",
    "    seq_out = []\n",
    "    for idx in range(max_length):\n",
    "        outputs = model(**inputs, return_dict=False, use_cache=use_cache)\n",
    "\n",
    "        if \"attention_mask\" in inputs:\n",
    "            del inputs[\"attention_mask\"]\n",
    "\n",
    "        next_token = outputs[0][0][:, -1].argmax(-1).unsqueeze(-1)\n",
    "        if use_cache:\n",
    "            inputs[\"past_key_values\"] = outputs[1]\n",
    "            inputs[\"ensemble_past_key_values\"] = outputs[2]\n",
    "            inputs[\"input_ids\"] = next_token\n",
    "        else:\n",
    "            inputs[\"input_ids\"] = torch.cat([inputs[\"input_ids\"], next_token], dim=1)\n",
    "        seq_out.append(next_token)\n",
    "\n",
    "    return torch.cat(seq_out, -1)\n",
    "\n",
    "inputs = [inputs_a, inputs_b, inputs_c, inputs_d]\n",
    "inputs = experiment.tokenizer(inputs, padding=True, return_tensors=\"pt\")\n",
    "seq_out = generate(experiment.model, inputs.to(\"cuda\"), use_cache=True)\n",
    "experiment.tokenizer.batch_decode(seq_out, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "posteriors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
